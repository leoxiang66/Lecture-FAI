# Intelligent Agents

## Agents and Environments
**Intelligent Agent**
An intelligent agent is anything that

- perceives its environment through sensors and
- acts upon the environment through actuators.

![](https://i.imgur.com/8fHi5xn.png)

### Compared to a control system view
In control theory, one typically distinguishes between 

- the system one wants to control
- the environment. 

In the Al setting, this distinction is often not made.

### Vaccum-cleaner world 吸尘器世界
![](https://i.imgur.com/LQGI937.png)

Percepts: location and contents, e.g., [ A, Dirty]

Actions: Left, Right, Suck, NoOp (No Operation)

### Agent function (Policy)
**Percept sequence**
An agent's percept sequence is the complete history of its perception.

- Vacuum cleaner example: $[A$, Dirty $],[A$, Clean $],[B$, Clean $],[A$, Clean $]$.

**Agent function**
An agent function maps any given percept sequence to an action.

- The behavior of an agent can be fully described by its agent function

**Tabular agent function**
![](https://i.imgur.com/IVYEs1R.png)

#### Analysis
![](https://i.imgur.com/S5LYgf1.png)



## The Concept of Rationality
### Rational agent
**Rationality**
A system is rational if it does the "right thing", i.e., has an ideal performance.

- An obvious performance measure is not always available.
- A designer has to find an acceptable measure.

![](https://i.imgur.com/89v2SOn.png)

**Rational agent**
For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the prior percept sequence and its built-in knowledge.

- A rational agent $(\neq$ omniscient agent $)$ maximizes **expected** performance.

**Omniscient agent 全知代理**
An omniscient agent knows the actual outcome of its actions, which is impossible in reality.


**Learning**
Rational agents are able to learn from perception, i.e., they improve their knowledge of the environment over time.

**Autonomy 自主**
In AI, a rational agent is considered more autonomous if it is less dependent on prior knowledge and uses newly learned abilities instead.

## The Nature of Environments

### Task Environment
To design a rational agent, we have to specify the task environment. We use the PEAS (**p**erformance, **e**nvironment, **a**ctuators, **s**ensors) description

#### examples
![](https://i.imgur.com/Y4pej73.png)
![](https://i.imgur.com/tqrjUka.png)


#### Properties of task environments
1. **Fully observable vs. partially observable**
An environment is fully observable if the agent can detect the complete state of the environment, and partially observable otherwise.
    - Example: The vacuum-cleaner world is partially observable since the robot only knows whether the current square is dirty.
2. **Single agent vs. multi agent**
An environment is a multi agent environment if it contains several agents, and a single agent environment otherwise.
    - Example: The vacuum-cleaner world is a single agent environment. A chess game is a two-agent environment.
3. **Deterministic vs. stochastic**
An environment is deterministic if its next state is fully determined by its 1) current state and 2) the action of the agent, 
Stochastic's otherwise.
    - Example: The automated taxi driver environment is stochastic since the behavior of other traffic participants is unpredictable. The outcome of a calculator is deterministic.
4. **Episodic vs. sequential**
An environment is episodic if the actions taken in one episode (in which the robot senses and acts) does not affect later episodes, and sequential otherwise.
    - Example: Detecting defective parts on a conveyor belt is episodic. Chess and automated taxi driving are sequential.
5. **Discrete vs. continuous**
![](https://i.imgur.com/WRarZNp.png)
6. **Static vs. dynamic**
If an environment only changes based on actions of the agent, it is static, and dynamic otherwise.
    - Example: The automated taxi driver environment is dynamic. A crossword puzzle is static.
7. **Known vs. unknown**
An environment is known if the agent knows the outcomes (or outcome probabilities) of its actions, and unknown otherwise. In the latter case, the agent has to learn the environment first.
    - Example: The agent knows all the rules of a card game it should play, thus it is in a known environment.

#### Examples of task environments
![](https://i.imgur.com/INQtbq8.png)


## The Structure of Agents
**Agent Types**
Besides categorizing the task environment, we also categorize agents in four categories with increasing generality:

- simple reflex反射 agents,
- reflex agents with state,
- goal-based agents,
- utility-based agents.

All these can be turned into learning agents.

### Simple reflex agents
![](https://i.imgur.com/OS8F79e.png)


### Model-based reflex agents
![](https://i.imgur.com/r7EmENk.png)
![](https://i.imgur.com/PQjuHcu.png)


### Goal-based agents
![](https://i.imgur.com/qlQvHBF.png)
![](https://i.imgur.com/ueTaIcc.png)

### Utility-based agents
![](https://i.imgur.com/8ZMbOFO.png)
![](https://i.imgur.com/84JC108.png)


### Learning agents
![](https://i.imgur.com/D27FEcv.png)
![](https://i.imgur.com/jVsJcrB.png)


## Summary
![](https://i.imgur.com/WXXG3yy.png)
