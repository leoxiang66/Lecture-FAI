{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Author:Tao Xiang Contact: tao.xiang@tum.de","title":"Welcome"},{"location":"#welcome","text":"Author:Tao Xiang Contact: tao.xiang@tum.de","title":"Welcome"},{"location":"1-Introduction/1.1%20introduction/","text":"Introduction What is AI? no clear definition Acting humanly The Turing test proposed by Alan Turing in 1950 tests whether a machine acts humanly. Turing test A computer passes the Turing test if a human interrogator cannot tell whether the answers from his written questions come from a human or a computer. The computer would need the following capabilities: natural language processing to communicate in natural language; knowledge representation to store what it knows or hears; automated reasoning to use stored information and to draw new conclusions; machine learning to adapt to new circumstances and to detect and explore patterns. Total Turing test Based on the Turing test and additionally requires that the subject can physically interact and see the other person using video. The computer would additionally need the following capabilities: computer vision to perceive objects; robotics to manipulate objects and move them. Thingking humanly Thinking humanly approach Once one has a sufficiently precise theory of the mind, one could write a computer program of it. If the input-output behavior matches human behavior, some inner workings of the program might correspond to human thinking. This approach is typically followed in cognitive sciences. Thinking rationally Rationality A system is rational if it does the \"right thing\", i.e., has an ideal performance (performance measures are not always available). Logics is often used to mimic rational thinking since it provides correct conclusions given correct premises, e.g.: Socrates is a man; all men are mortal; therefore, Socrates is mortal. Difficulties: Informal knowledge has to be formalized. Computational effort might be infeasible: Problems with a few hundred facts might exhaust the capabilities of today's computers. Acting rationally Agent An agent is just something that acts (Latin: agere, \"to do\" ). Rational agent A rational agent is one that acts so as to achieve the best outcome. Topics of this lecture Search: Problem is deterministic and goal state can be checked. Constraint satisfaction: It is only required to find a solution fulfilling constraints. Logics: Problem is deterministic, knowledge is given, and new knowledge should be inferred. Probabilistic reasoning: Problem is stochastic, knowledge is uncertain, model is given, and state of agent should be estimated. Rational decisions: Problem is stochastic, knowledge is uncertain, model is given, and best action of agent should be determined. Learning: Problem is stochastic, knowledge is uncertain, data/simulation instead of model is given, and best action of agent should be determined. This classification is not sharp. For instance, there are also search approaches for stochastic problems. Summary","title":"Introduction"},{"location":"1-Introduction/1.1%20introduction/#introduction","text":"","title":"Introduction"},{"location":"1-Introduction/1.1%20introduction/#what-is-ai","text":"no clear definition","title":"What is AI?"},{"location":"1-Introduction/1.1%20introduction/#acting-humanly","text":"The Turing test proposed by Alan Turing in 1950 tests whether a machine acts humanly. Turing test A computer passes the Turing test if a human interrogator cannot tell whether the answers from his written questions come from a human or a computer. The computer would need the following capabilities: natural language processing to communicate in natural language; knowledge representation to store what it knows or hears; automated reasoning to use stored information and to draw new conclusions; machine learning to adapt to new circumstances and to detect and explore patterns. Total Turing test Based on the Turing test and additionally requires that the subject can physically interact and see the other person using video. The computer would additionally need the following capabilities: computer vision to perceive objects; robotics to manipulate objects and move them.","title":"Acting humanly"},{"location":"1-Introduction/1.1%20introduction/#thingking-humanly","text":"Thinking humanly approach Once one has a sufficiently precise theory of the mind, one could write a computer program of it. If the input-output behavior matches human behavior, some inner workings of the program might correspond to human thinking. This approach is typically followed in cognitive sciences.","title":"Thingking humanly"},{"location":"1-Introduction/1.1%20introduction/#thinking-rationally","text":"Rationality A system is rational if it does the \"right thing\", i.e., has an ideal performance (performance measures are not always available). Logics is often used to mimic rational thinking since it provides correct conclusions given correct premises, e.g.: Socrates is a man; all men are mortal; therefore, Socrates is mortal. Difficulties: Informal knowledge has to be formalized. Computational effort might be infeasible: Problems with a few hundred facts might exhaust the capabilities of today's computers.","title":"Thinking rationally"},{"location":"1-Introduction/1.1%20introduction/#acting-rationally","text":"Agent An agent is just something that acts (Latin: agere, \"to do\" ). Rational agent A rational agent is one that acts so as to achieve the best outcome.","title":"Acting rationally"},{"location":"1-Introduction/1.1%20introduction/#topics-of-this-lecture","text":"Search: Problem is deterministic and goal state can be checked. Constraint satisfaction: It is only required to find a solution fulfilling constraints. Logics: Problem is deterministic, knowledge is given, and new knowledge should be inferred. Probabilistic reasoning: Problem is stochastic, knowledge is uncertain, model is given, and state of agent should be estimated. Rational decisions: Problem is stochastic, knowledge is uncertain, model is given, and best action of agent should be determined. Learning: Problem is stochastic, knowledge is uncertain, data/simulation instead of model is given, and best action of agent should be determined. This classification is not sharp. For instance, there are also search approaches for stochastic problems.","title":"Topics of this lecture"},{"location":"1-Introduction/1.1%20introduction/#summary","text":"","title":"Summary"},{"location":"1-Introduction/1.2%20Agents/","text":"Intelligent Agents Agents and Environments Intelligent Agent An intelligent agent is anything that perceives its environment through sensors and acts upon the environment through actuators. Compared to a control system view In control theory, one typically distinguishes between the system one wants to control the environment. In the Al setting, this distinction is often not made. Vaccum-cleaner world \u5438\u5c18\u5668\u4e16\u754c Percepts: location and contents, e.g., [ A, Dirty] Actions: Left, Right, Suck, NoOp (No Operation) Agent function (Policy) Percept sequence An agent's percept sequence is the complete history of its perception. Vacuum cleaner example: \\([A\\) , Dirty \\(],[A\\) , Clean \\(],[B\\) , Clean \\(],[A\\) , Clean \\(]\\) . Agent function An agent function maps any given percept sequence to an action. The behavior of an agent can be fully described by its agent function Tabular agent function Analysis The Concept of Rationality Rational agent Rationality A system is rational if it does the \"right thing\", i.e., has an ideal performance. An obvious performance measure is not always available. A designer has to find an acceptable measure. Rational agent For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the prior percept sequence and its built-in knowledge. A rational agent \\((\\neq\\) omniscient agent \\()\\) maximizes expected performance. Omniscient agent \u5168\u77e5\u4ee3\u7406 An omniscient agent knows the actual outcome of its actions, which is impossible in reality. Learning Rational agents are able to learn from perception, i.e., they improve their knowledge of the environment over time. Autonomy \u81ea\u4e3b In AI, a rational agent is considered more autonomous if it is less dependent on prior knowledge and uses newly learned abilities instead. The Nature of Environments Task Environment To design a rational agent, we have to specify the task environment. We use the PEAS ( p erformance, e nvironment, a ctuators, s ensors) description examples Properties of task environments Fully observable vs. partially observable An environment is fully observable if the agent can detect the complete state of the environment, and partially observable otherwise. Example: The vacuum-cleaner world is partially observable since the robot only knows whether the current square is dirty. Single agent vs. multi agent An environment is a multi agent environment if it contains several agents, and a single agent environment otherwise. Example: The vacuum-cleaner world is a single agent environment. A chess game is a two-agent environment. Deterministic vs. stochastic An environment is deterministic if its next state is fully determined by its 1) current state and 2) the action of the agent, Stochastic's otherwise. Example: The automated taxi driver environment is stochastic since the behavior of other traffic participants is unpredictable. The outcome of a calculator is deterministic. Episodic vs. sequential An environment is episodic if the actions taken in one episode (in which the robot senses and acts) does not affect later episodes, and sequential otherwise. Example: Detecting defective parts on a conveyor belt is episodic. Chess and automated taxi driving are sequential. Discrete vs. continuous Static vs. dynamic If an environment only changes based on actions of the agent, it is static, and dynamic otherwise. Example: The automated taxi driver environment is dynamic. A crossword puzzle is static. Known vs. unknown An environment is known if the agent knows the outcomes (or outcome probabilities) of its actions, and unknown otherwise. In the latter case, the agent has to learn the environment first. Example: The agent knows all the rules of a card game it should play, thus it is in a known environment. Examples of task environments The Structure of Agents Agent Types Besides categorizing the task environment, we also categorize agents in four categories with increasing generality: simple reflex\u53cd\u5c04 agents, reflex agents with state, goal-based agents, utility-based agents. All these can be turned into learning agents. Simple reflex agents Model-based reflex agents Goal-based agents Utility-based agents Learning agents Summary","title":"Intelligent Agents"},{"location":"1-Introduction/1.2%20Agents/#intelligent-agents","text":"","title":"Intelligent Agents"},{"location":"1-Introduction/1.2%20Agents/#agents-and-environments","text":"Intelligent Agent An intelligent agent is anything that perceives its environment through sensors and acts upon the environment through actuators.","title":"Agents and Environments"},{"location":"1-Introduction/1.2%20Agents/#compared-to-a-control-system-view","text":"In control theory, one typically distinguishes between the system one wants to control the environment. In the Al setting, this distinction is often not made.","title":"Compared to a control system view"},{"location":"1-Introduction/1.2%20Agents/#vaccum-cleaner-world","text":"Percepts: location and contents, e.g., [ A, Dirty] Actions: Left, Right, Suck, NoOp (No Operation)","title":"Vaccum-cleaner world \u5438\u5c18\u5668\u4e16\u754c"},{"location":"1-Introduction/1.2%20Agents/#agent-function-policy","text":"Percept sequence An agent's percept sequence is the complete history of its perception. Vacuum cleaner example: \\([A\\) , Dirty \\(],[A\\) , Clean \\(],[B\\) , Clean \\(],[A\\) , Clean \\(]\\) . Agent function An agent function maps any given percept sequence to an action. The behavior of an agent can be fully described by its agent function Tabular agent function","title":"Agent function (Policy)"},{"location":"1-Introduction/1.2%20Agents/#analysis","text":"","title":"Analysis"},{"location":"1-Introduction/1.2%20Agents/#the-concept-of-rationality","text":"","title":"The Concept of Rationality"},{"location":"1-Introduction/1.2%20Agents/#rational-agent","text":"Rationality A system is rational if it does the \"right thing\", i.e., has an ideal performance. An obvious performance measure is not always available. A designer has to find an acceptable measure. Rational agent For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the prior percept sequence and its built-in knowledge. A rational agent \\((\\neq\\) omniscient agent \\()\\) maximizes expected performance. Omniscient agent \u5168\u77e5\u4ee3\u7406 An omniscient agent knows the actual outcome of its actions, which is impossible in reality. Learning Rational agents are able to learn from perception, i.e., they improve their knowledge of the environment over time. Autonomy \u81ea\u4e3b In AI, a rational agent is considered more autonomous if it is less dependent on prior knowledge and uses newly learned abilities instead.","title":"Rational agent"},{"location":"1-Introduction/1.2%20Agents/#the-nature-of-environments","text":"","title":"The Nature of Environments"},{"location":"1-Introduction/1.2%20Agents/#task-environment","text":"To design a rational agent, we have to specify the task environment. We use the PEAS ( p erformance, e nvironment, a ctuators, s ensors) description","title":"Task Environment"},{"location":"1-Introduction/1.2%20Agents/#examples","text":"","title":"examples"},{"location":"1-Introduction/1.2%20Agents/#properties-of-task-environments","text":"Fully observable vs. partially observable An environment is fully observable if the agent can detect the complete state of the environment, and partially observable otherwise. Example: The vacuum-cleaner world is partially observable since the robot only knows whether the current square is dirty. Single agent vs. multi agent An environment is a multi agent environment if it contains several agents, and a single agent environment otherwise. Example: The vacuum-cleaner world is a single agent environment. A chess game is a two-agent environment. Deterministic vs. stochastic An environment is deterministic if its next state is fully determined by its 1) current state and 2) the action of the agent, Stochastic's otherwise. Example: The automated taxi driver environment is stochastic since the behavior of other traffic participants is unpredictable. The outcome of a calculator is deterministic. Episodic vs. sequential An environment is episodic if the actions taken in one episode (in which the robot senses and acts) does not affect later episodes, and sequential otherwise. Example: Detecting defective parts on a conveyor belt is episodic. Chess and automated taxi driving are sequential. Discrete vs. continuous Static vs. dynamic If an environment only changes based on actions of the agent, it is static, and dynamic otherwise. Example: The automated taxi driver environment is dynamic. A crossword puzzle is static. Known vs. unknown An environment is known if the agent knows the outcomes (or outcome probabilities) of its actions, and unknown otherwise. In the latter case, the agent has to learn the environment first. Example: The agent knows all the rules of a card game it should play, thus it is in a known environment.","title":"Properties of task environments"},{"location":"1-Introduction/1.2%20Agents/#examples-of-task-environments","text":"","title":"Examples of task environments"},{"location":"1-Introduction/1.2%20Agents/#the-structure-of-agents","text":"Agent Types Besides categorizing the task environment, we also categorize agents in four categories with increasing generality: simple reflex\u53cd\u5c04 agents, reflex agents with state, goal-based agents, utility-based agents. All these can be turned into learning agents.","title":"The Structure of Agents"},{"location":"1-Introduction/1.2%20Agents/#simple-reflex-agents","text":"","title":"Simple reflex agents"},{"location":"1-Introduction/1.2%20Agents/#model-based-reflex-agents","text":"","title":"Model-based reflex agents"},{"location":"1-Introduction/1.2%20Agents/#goal-based-agents","text":"","title":"Goal-based agents"},{"location":"1-Introduction/1.2%20Agents/#utility-based-agents","text":"","title":"Utility-based agents"},{"location":"1-Introduction/1.2%20Agents/#learning-agents","text":"","title":"Learning agents"},{"location":"1-Introduction/1.2%20Agents/#summary","text":"","title":"Summary"}]}