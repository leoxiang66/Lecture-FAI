{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Author:Tao Xiang Contact: tao.xiang@tum.de","title":"Welcome"},{"location":"#welcome","text":"Author:Tao Xiang Contact: tao.xiang@tum.de","title":"Welcome"},{"location":"1-Introduction/1.1%20introduction/","text":"Introduction What is AI? no clear definition Acting humanly The Turing test proposed by Alan Turing in 1950 tests whether a machine acts humanly. Turing test A computer passes the Turing test if a human interrogator cannot tell whether the answers from his written questions come from a human or a computer. The computer would need the following capabilities: natural language processing to communicate in natural language; knowledge representation to store what it knows or hears; automated reasoning to use stored information and to draw new conclusions; machine learning to adapt to new circumstances and to detect and explore patterns. Total Turing test Based on the Turing test and additionally requires that the subject can physically interact and see the other person using video. The computer would additionally need the following capabilities: computer vision to perceive objects; robotics to manipulate objects and move them. Thingking humanly Thinking humanly approach Once one has a sufficiently precise theory of the mind, one could write a computer program of it. If the input-output behavior matches human behavior, some inner workings of the program might correspond to human thinking. This approach is typically followed in cognitive sciences. Thinking rationally Rationality A system is rational if it does the \"right thing\", i.e., has an ideal performance (performance measures are not always available). Logics is often used to mimic rational thinking since it provides correct conclusions given correct premises, e.g.: Socrates is a man; all men are mortal; therefore, Socrates is mortal. Difficulties: Informal knowledge has to be formalized. Computational effort might be infeasible: Problems with a few hundred facts might exhaust the capabilities of today's computers. Acting rationally Agent An agent is just something that acts (Latin: agere, \"to do\" ). Rational agent A rational agent is one that acts so as to achieve the best outcome. Topics of this lecture Search: Problem is deterministic and goal state can be checked. Constraint satisfaction: It is only required to find a solution fulfilling constraints. Logics: Problem is deterministic, knowledge is given, and new knowledge should be inferred. Probabilistic reasoning: Problem is stochastic, knowledge is uncertain, model is given, and state of agent should be estimated. Rational decisions: Problem is stochastic, knowledge is uncertain, model is given, and best action of agent should be determined. Learning: Problem is stochastic, knowledge is uncertain, data/simulation instead of model is given, and best action of agent should be determined. This classification is not sharp. For instance, there are also search approaches for stochastic problems. Summary","title":"Introduction"},{"location":"1-Introduction/1.1%20introduction/#introduction","text":"","title":"Introduction"},{"location":"1-Introduction/1.1%20introduction/#what-is-ai","text":"no clear definition","title":"What is AI?"},{"location":"1-Introduction/1.1%20introduction/#acting-humanly","text":"The Turing test proposed by Alan Turing in 1950 tests whether a machine acts humanly. Turing test A computer passes the Turing test if a human interrogator cannot tell whether the answers from his written questions come from a human or a computer. The computer would need the following capabilities: natural language processing to communicate in natural language; knowledge representation to store what it knows or hears; automated reasoning to use stored information and to draw new conclusions; machine learning to adapt to new circumstances and to detect and explore patterns. Total Turing test Based on the Turing test and additionally requires that the subject can physically interact and see the other person using video. The computer would additionally need the following capabilities: computer vision to perceive objects; robotics to manipulate objects and move them.","title":"Acting humanly"},{"location":"1-Introduction/1.1%20introduction/#thingking-humanly","text":"Thinking humanly approach Once one has a sufficiently precise theory of the mind, one could write a computer program of it. If the input-output behavior matches human behavior, some inner workings of the program might correspond to human thinking. This approach is typically followed in cognitive sciences.","title":"Thingking humanly"},{"location":"1-Introduction/1.1%20introduction/#thinking-rationally","text":"Rationality A system is rational if it does the \"right thing\", i.e., has an ideal performance (performance measures are not always available). Logics is often used to mimic rational thinking since it provides correct conclusions given correct premises, e.g.: Socrates is a man; all men are mortal; therefore, Socrates is mortal. Difficulties: Informal knowledge has to be formalized. Computational effort might be infeasible: Problems with a few hundred facts might exhaust the capabilities of today's computers.","title":"Thinking rationally"},{"location":"1-Introduction/1.1%20introduction/#acting-rationally","text":"Agent An agent is just something that acts (Latin: agere, \"to do\" ). Rational agent A rational agent is one that acts so as to achieve the best outcome.","title":"Acting rationally"},{"location":"1-Introduction/1.1%20introduction/#topics-of-this-lecture","text":"Search: Problem is deterministic and goal state can be checked. Constraint satisfaction: It is only required to find a solution fulfilling constraints. Logics: Problem is deterministic, knowledge is given, and new knowledge should be inferred. Probabilistic reasoning: Problem is stochastic, knowledge is uncertain, model is given, and state of agent should be estimated. Rational decisions: Problem is stochastic, knowledge is uncertain, model is given, and best action of agent should be determined. Learning: Problem is stochastic, knowledge is uncertain, data/simulation instead of model is given, and best action of agent should be determined. This classification is not sharp. For instance, there are also search approaches for stochastic problems.","title":"Topics of this lecture"},{"location":"1-Introduction/1.1%20introduction/#summary","text":"","title":"Summary"},{"location":"1-Introduction/1.2%20Agents/","text":"Intelligent Agents Agents and Environments Intelligent Agent An intelligent agent is anything that perceives its environment through sensors and acts upon the environment through actuators. Compared to a control system view In control theory, one typically distinguishes between the system one wants to control the environment. In the Al setting, this distinction is often not made. Vaccum-cleaner world \u5438\u5c18\u5668\u4e16\u754c Percepts: location and contents, e.g., [ A, Dirty] Actions: Left, Right, Suck, NoOp (No Operation) Agent function (Policy) Percept sequence An agent's percept sequence is the complete history of its perception. Vacuum cleaner example: \\([A\\) , Dirty \\(],[A\\) , Clean \\(],[B\\) , Clean \\(],[A\\) , Clean \\(]\\) . Agent function An agent function maps any given percept sequence to an action. The behavior of an agent can be fully described by its agent function Tabular agent function Analysis The Concept of Rationality Rational agent Rationality A system is rational if it does the \"right thing\", i.e., has an ideal performance. An obvious performance measure is not always available. A designer has to find an acceptable measure. Rational agent For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the prior percept sequence and its built-in knowledge. A rational agent \\((\\neq\\) omniscient agent \\()\\) maximizes expected performance. Omniscient agent \u5168\u77e5\u4ee3\u7406 An omniscient agent knows the actual outcome of its actions, which is impossible in reality. Learning Rational agents are able to learn from perception, i.e., they improve their knowledge of the environment over time. Autonomy \u81ea\u4e3b In AI, a rational agent is considered more autonomous if it is less dependent on prior knowledge and uses newly learned abilities instead. The Nature of Environments Task Environment To design a rational agent, we have to specify the task environment. We use the PEAS ( p erformance, e nvironment, a ctuators, s ensors) description examples Properties of task environments Fully observable vs. partially observable An environment is fully observable if the agent can detect the complete state of the environment, and partially observable otherwise. Example: The vacuum-cleaner world is partially observable since the robot only knows whether the current square is dirty. Single agent vs. multi agent An environment is a multi agent environment if it contains several agents, and a single agent environment otherwise. Example: The vacuum-cleaner world is a single agent environment. A chess game is a two-agent environment. Deterministic vs. stochastic An environment is deterministic if its next state is fully determined by its 1) current state and 2) the action of the agent, Stochastic's otherwise. Example: The automated taxi driver environment is stochastic since the behavior of other traffic participants is unpredictable. The outcome of a calculator is deterministic. Episodic vs. sequential An environment is episodic if the actions taken in one episode (in which the robot senses and acts) does not affect later episodes, and sequential otherwise. Example: Detecting defective parts on a conveyor belt is episodic. Chess and automated taxi driving are sequential. Discrete vs. continuous Static vs. dynamic If an environment only changes based on actions of the agent, it is static, and dynamic otherwise. Example: The automated taxi driver environment is dynamic. A crossword puzzle is static. Known vs. unknown An environment is known if the agent knows the outcomes (or outcome probabilities) of its actions, and unknown otherwise. In the latter case, the agent has to learn the environment first. Example: The agent knows all the rules of a card game it should play, thus it is in a known environment. Examples of task environments The Structure of Agents Agent Types Besides categorizing the task environment, we also categorize agents in four categories with increasing generality: simple reflex\u53cd\u5c04 agents, reflex agents with state, goal-based agents, utility-based agents. All these can be turned into learning agents. Simple reflex agents Model-based reflex agents Goal-based agents Utility-based agents Learning agents Summary","title":"Intelligent Agents"},{"location":"1-Introduction/1.2%20Agents/#intelligent-agents","text":"","title":"Intelligent Agents"},{"location":"1-Introduction/1.2%20Agents/#agents-and-environments","text":"Intelligent Agent An intelligent agent is anything that perceives its environment through sensors and acts upon the environment through actuators.","title":"Agents and Environments"},{"location":"1-Introduction/1.2%20Agents/#compared-to-a-control-system-view","text":"In control theory, one typically distinguishes between the system one wants to control the environment. In the Al setting, this distinction is often not made.","title":"Compared to a control system view"},{"location":"1-Introduction/1.2%20Agents/#vaccum-cleaner-world","text":"Percepts: location and contents, e.g., [ A, Dirty] Actions: Left, Right, Suck, NoOp (No Operation)","title":"Vaccum-cleaner world \u5438\u5c18\u5668\u4e16\u754c"},{"location":"1-Introduction/1.2%20Agents/#agent-function-policy","text":"Percept sequence An agent's percept sequence is the complete history of its perception. Vacuum cleaner example: \\([A\\) , Dirty \\(],[A\\) , Clean \\(],[B\\) , Clean \\(],[A\\) , Clean \\(]\\) . Agent function An agent function maps any given percept sequence to an action. The behavior of an agent can be fully described by its agent function Tabular agent function","title":"Agent function (Policy)"},{"location":"1-Introduction/1.2%20Agents/#analysis","text":"","title":"Analysis"},{"location":"1-Introduction/1.2%20Agents/#the-concept-of-rationality","text":"","title":"The Concept of Rationality"},{"location":"1-Introduction/1.2%20Agents/#rational-agent","text":"Rationality A system is rational if it does the \"right thing\", i.e., has an ideal performance. An obvious performance measure is not always available. A designer has to find an acceptable measure. Rational agent For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the prior percept sequence and its built-in knowledge. A rational agent \\((\\neq\\) omniscient agent \\()\\) maximizes expected performance. Omniscient agent \u5168\u77e5\u4ee3\u7406 An omniscient agent knows the actual outcome of its actions, which is impossible in reality. Learning Rational agents are able to learn from perception, i.e., they improve their knowledge of the environment over time. Autonomy \u81ea\u4e3b In AI, a rational agent is considered more autonomous if it is less dependent on prior knowledge and uses newly learned abilities instead.","title":"Rational agent"},{"location":"1-Introduction/1.2%20Agents/#the-nature-of-environments","text":"","title":"The Nature of Environments"},{"location":"1-Introduction/1.2%20Agents/#task-environment","text":"To design a rational agent, we have to specify the task environment. We use the PEAS ( p erformance, e nvironment, a ctuators, s ensors) description","title":"Task Environment"},{"location":"1-Introduction/1.2%20Agents/#examples","text":"","title":"examples"},{"location":"1-Introduction/1.2%20Agents/#properties-of-task-environments","text":"Fully observable vs. partially observable An environment is fully observable if the agent can detect the complete state of the environment, and partially observable otherwise. Example: The vacuum-cleaner world is partially observable since the robot only knows whether the current square is dirty. Single agent vs. multi agent An environment is a multi agent environment if it contains several agents, and a single agent environment otherwise. Example: The vacuum-cleaner world is a single agent environment. A chess game is a two-agent environment. Deterministic vs. stochastic An environment is deterministic if its next state is fully determined by its 1) current state and 2) the action of the agent, Stochastic's otherwise. Example: The automated taxi driver environment is stochastic since the behavior of other traffic participants is unpredictable. The outcome of a calculator is deterministic. Episodic vs. sequential An environment is episodic if the actions taken in one episode (in which the robot senses and acts) does not affect later episodes, and sequential otherwise. Example: Detecting defective parts on a conveyor belt is episodic. Chess and automated taxi driving are sequential. Discrete vs. continuous Static vs. dynamic If an environment only changes based on actions of the agent, it is static, and dynamic otherwise. Example: The automated taxi driver environment is dynamic. A crossword puzzle is static. Known vs. unknown An environment is known if the agent knows the outcomes (or outcome probabilities) of its actions, and unknown otherwise. In the latter case, the agent has to learn the environment first. Example: The agent knows all the rules of a card game it should play, thus it is in a known environment.","title":"Properties of task environments"},{"location":"1-Introduction/1.2%20Agents/#examples-of-task-environments","text":"","title":"Examples of task environments"},{"location":"1-Introduction/1.2%20Agents/#the-structure-of-agents","text":"Agent Types Besides categorizing the task environment, we also categorize agents in four categories with increasing generality: simple reflex\u53cd\u5c04 agents, reflex agents with state, goal-based agents, utility-based agents. All these can be turned into learning agents.","title":"The Structure of Agents"},{"location":"1-Introduction/1.2%20Agents/#simple-reflex-agents","text":"","title":"Simple reflex agents"},{"location":"1-Introduction/1.2%20Agents/#model-based-reflex-agents","text":"","title":"Model-based reflex agents"},{"location":"1-Introduction/1.2%20Agents/#goal-based-agents","text":"","title":"Goal-based agents"},{"location":"1-Introduction/1.2%20Agents/#utility-based-agents","text":"","title":"Utility-based agents"},{"location":"1-Introduction/1.2%20Agents/#learning-agents","text":"","title":"Learning agents"},{"location":"1-Introduction/1.2%20Agents/#summary","text":"","title":"Summary"},{"location":"2-Topic%3A%20Search%20Algorithm/2.1%20Intro/","text":"Problem Solving This chapter describes one kind of goal-based agent called a problem-solving agent . Problem-solving agents use atomic representations, as described in Section 2.4.7-that is, states of the world are considered as wholes, with no internal structure visible to the problemsolving algorithms. Goal-based agents that use more advanced factored or structured representations are usually called planning agents and are discussed in Chapters 7 and 10. Content of this topic: precise definitions of problems and their solutions and give several examples to illustrate these definitions. We then describe several general-purpose search algorithms that can be used to solve these problems. We will see several uninformed search algorithms that are given no information about the problem other than its definition. Although some of these algorithms can solve any solvable problem, none of them can do so efficiently. Informed search algorithms , on the other hand, can do quite well given some guidance on where to look for solutions.","title":"Problem Solving"},{"location":"2-Topic%3A%20Search%20Algorithm/2.1%20Intro/#problem-solving","text":"This chapter describes one kind of goal-based agent called a problem-solving agent . Problem-solving agents use atomic representations, as described in Section 2.4.7-that is, states of the world are considered as wholes, with no internal structure visible to the problemsolving algorithms. Goal-based agents that use more advanced factored or structured representations are usually called planning agents and are discussed in Chapters 7 and 10. Content of this topic: precise definitions of problems and their solutions and give several examples to illustrate these definitions. We then describe several general-purpose search algorithms that can be used to solve these problems. We will see several uninformed search algorithms that are given no information about the problem other than its definition. Although some of these algorithms can solve any solvable problem, none of them can do so efficiently. Informed search algorithms , on the other hand, can do quite well given some guidance on where to look for solutions.","title":"Problem Solving"},{"location":"2-Topic%3A%20Search%20Algorithm/2.2%20Problem%20Solving%20Agents/","text":"Problem Solving Agents \u4e00\u4e2aagent\u9700\u8981\u6700\u5927\u5316\u4ed6\u7684performance measure. \u5982\u679c\u7ed9\u8fd9\u4e2aagent\u8bbe\u8ba1\u4e00\u4e2a(\u597d\u7684)goal, \u8fd9\u4f1a\u6700\u5927\u5316performance measure\u53d8\u5f97\u5bb9\u6613\u3002 Problem Solving \u7b2c\u4e00\u6b65\uff1a Goal Formulation based on the current situation and the agent's performance \u901a\u5e38\uff0c goals are defined as a set of world states example goal: be in city Munich Problem Solving \u7b2c\u4e8c\u6b65\uff1a Problem Formulation \u5728\u786e\u5b9agoal\u4ee5\u540e\uff0c \u6211\u4eec\u9700\u8981\u5b9a\u4e49\u53ef\u7528\u7684actions\u548cstates. Problem formulation is the process of deciding what actions and states to consider, given a goal. actions: go from one city to other states: be in a certain city Our agent has now adopted the goal of driving to Bucharest and is considering where to go from Arad. Three roads lead out of Arad, one toward Sibiu, one to Timisoara, and one to Zerind. None of these achieves the goal, so unless the agent is familiar with the geography of Romania, it will not know which road to follow. \\({ }^1\\) In other words, the agent will not know which of its possible actions is best, because it does not yet know enough about the state that results from taking each action. If the agent has no additional information-i.e., if the environment is unknown in the sense defined in Section 2.3-then it is has no choice but to try one of the actions at random. This sad situation is discussed in Chapter 4 . Assumptions: the environment is known: agent knows which states are reached by each action the environment is observable \u4ee3\u7406\u53ef\u4ee5\u77e5\u9053\u5b83\u5f53\u524d\u7684state the environment is discrete: at any given state there are only finitely many actions to choose from the environment is deterministic: \u67d0\u4e2aaction\u90fd\u6709\u4e00\u4e2a\u786e\u5b9a\u7684\u7ed3\u679c Problem Solving \u7b2c\u4e09\u6b65: Search solution to problems: a fixed sequence of actions in general it could be a branching strategy that recommends different actions in the future depending on what percepts arrive (IF/ELSE) Search: The process of looking for a sequence of actions that reaches the goal is called search . A search algorithm takes a problem as input and returns a solution in the form of an action sequence. Problem Solving \u7b2c\u56db\u6b65: Execution Once a solution is found, the actions it recommends can be carried out. This is called the execution phase. Thus, we have a simple \"formulate, search, execute\" design for the agent, as shown in Figure 3.1. After formulating a goal and a problem to solve, the agent calls a search procedure to solve it. It then uses the solution to guide its actions, doing whatever the solution recommends as the next thing to do-typically, the first action of the sequence-and then removing that step from the sequence. Once the solution has been executed, the agent will formulate a new goal. Open-Loop: execution\u8fc7\u7a0b\u4e2d\u4e0d\u770bpercepts. Notice that while the agent is executing the solution sequence it ignores its percepts when choosing an action because it knows in advance what they will be. An agent that carries out its plans with its eyes closed, so to speak, must be quite certain of what is going on. Control theorists call this an open-loop system, because ignoring the percepts breaks the loop between agent and environment. Well-defined problems and solutions A problem can be defined formally by five components: The initial state that the agent starts in A description of the possible actions available to the agent. Given a particular state \\(s\\) , ACTIONS \\((s)\\) returns the set of actions that can be executed in \\(s\\) . We say that each of these actions is applicable in \\(s\\) . A description of what each action does; the formal name for this is the transition model , specified by a function \\(\\operatorname{ResulT}(s, a)\\) that returns the state that results from doing action \\(a\\) in state \\(s\\) . We also use the term successor to refer to any state reachable from a given state by a single action. \\({ }^2\\) For example, we have Together, the initial state, actions, and transition model implicitly define the state space of the problem-the set of all states reachable from the initial state by any sequence of actions. The state space forms a directed network or graph in which the nodes are states and the links between nodes are actions. A path in the state space is a sequence of states connected by a sequence of actions. The goal test , which determines whether a given state is a goal state. Sometimes there is an explicit set of possible goal states, and the test simply checks whether the given state is one of them. A path cost function that assigns a numeric cost to each path. The problem-solving agent chooses a cost function that reflects its own performance measure. In this chapter, we assume that the cost of a path can be described as the sum of the costs of the individual actions along the path. \\({ }^3\\) The step cost of taking action \\(a\\) in state \\(s\\) to reach state \\(s^{\\prime}\\) is denoted by \\(c\\left(s, a, s^{\\prime}\\right)\\) . We assume that step costs are nonnegative. \\({ }^4\\) A solution to a problem is an action sequence that leads from the initial state to a goal state. Solution quality is measured by the path cost function, and an optimal solution has the lowest path cost among all solutions. Formulating Problems abstraction. The process of removing detail from a representation. \u73b0\u5b9e\u4e2d\u7684\u73af\u5883\u72b6\u6001\u5305\u542b\u592a\u591a\u4fe1\u606f\uff0c \u4f46\u662f\u5f88\u591a\u4fe1\u606f\u90fd\u662f\u4e0e\u8fd9\u4e2aproblem\u65e0\u5173\u7684\u3002 \u5220\u9664\u65e0\u5173\u4fe1\u606f => abstraction. \u9664\u4e86\u62bd\u8c61\u72b6\u6001\u63cf\u8ff0\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u5fc5\u987b\u62bd\u8c61\u52a8\u4f5c\u672c\u8eab\u3002 Abstraction is valid if we can expand any abstract solution into a solution in the more detailed world; a sufficient condition is that for every detailed state that is \"in Arad,\" there is a detailed path to some state that is \"in Sibiu,\" and so on. useful if carrying out each of the actions in the solution is easier than the original problem; in this case they are easy enough that they can be carried out without further search or planning by an average driving agent. The choice of a good abstraction thus involves removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out. Were it not for the ability to construct useful abstractions, intelligent agents would be completely swamped by the real world. Searching for Solutions A solution is an action sequence, so search algorithms work by considering various possible action sequences. The possible action sequences starting at the initial state form a search tree with the initial state at the root; the branches are actions and the nodes correspond to states in the state space of the problem. The root node of the tree corresponds to the initial state. The first step is to test whether this is a goal state. Then we need to consider taking various actions. We do this by expanding the current state; that is, applying each legal action to the current state, thereby generating a new set of states. In this case, we add three branches from the parent node \\(\\operatorname{In}(\\) Arad) leading to three new child nodes: In(Sibiu), In (Timisoara), and \\(\\operatorname{In}\\) (Zerind). Now we must choose which of these three possibilities to consider further. The set of all leaf nodes available for expansion at any given point is called the frontier The eagle-eyed reader will notice one peculiar thing about the search tree shown in Figure 3.6: it includes the path from Arad to Sibiu and back to Arad again! We say that In(Arad)is a repeated state in the search tree, generated in this case by a loopy path . Measure Problem Solving We can evaluate the performance of a search algorithm using the following criteria: Completeness: Is it guaranteed that the algorithm finds a solution if one exists? Optimality: Does the strategy find the optimal solution (minimum costs)? Time complexity: How long does it take to find a solution? Space complexity: How much memory is needed to perform the search? Infrastructure of Search Algorithms","title":"Problem Solving Agents"},{"location":"2-Topic%3A%20Search%20Algorithm/2.2%20Problem%20Solving%20Agents/#problem-solving-agents","text":"\u4e00\u4e2aagent\u9700\u8981\u6700\u5927\u5316\u4ed6\u7684performance measure. \u5982\u679c\u7ed9\u8fd9\u4e2aagent\u8bbe\u8ba1\u4e00\u4e2a(\u597d\u7684)goal, \u8fd9\u4f1a\u6700\u5927\u5316performance measure\u53d8\u5f97\u5bb9\u6613\u3002 Problem Solving \u7b2c\u4e00\u6b65\uff1a Goal Formulation based on the current situation and the agent's performance \u901a\u5e38\uff0c goals are defined as a set of world states example goal: be in city Munich Problem Solving \u7b2c\u4e8c\u6b65\uff1a Problem Formulation \u5728\u786e\u5b9agoal\u4ee5\u540e\uff0c \u6211\u4eec\u9700\u8981\u5b9a\u4e49\u53ef\u7528\u7684actions\u548cstates. Problem formulation is the process of deciding what actions and states to consider, given a goal. actions: go from one city to other states: be in a certain city Our agent has now adopted the goal of driving to Bucharest and is considering where to go from Arad. Three roads lead out of Arad, one toward Sibiu, one to Timisoara, and one to Zerind. None of these achieves the goal, so unless the agent is familiar with the geography of Romania, it will not know which road to follow. \\({ }^1\\) In other words, the agent will not know which of its possible actions is best, because it does not yet know enough about the state that results from taking each action. If the agent has no additional information-i.e., if the environment is unknown in the sense defined in Section 2.3-then it is has no choice but to try one of the actions at random. This sad situation is discussed in Chapter 4 . Assumptions: the environment is known: agent knows which states are reached by each action the environment is observable \u4ee3\u7406\u53ef\u4ee5\u77e5\u9053\u5b83\u5f53\u524d\u7684state the environment is discrete: at any given state there are only finitely many actions to choose from the environment is deterministic: \u67d0\u4e2aaction\u90fd\u6709\u4e00\u4e2a\u786e\u5b9a\u7684\u7ed3\u679c Problem Solving \u7b2c\u4e09\u6b65: Search solution to problems: a fixed sequence of actions in general it could be a branching strategy that recommends different actions in the future depending on what percepts arrive (IF/ELSE) Search: The process of looking for a sequence of actions that reaches the goal is called search . A search algorithm takes a problem as input and returns a solution in the form of an action sequence. Problem Solving \u7b2c\u56db\u6b65: Execution Once a solution is found, the actions it recommends can be carried out. This is called the execution phase. Thus, we have a simple \"formulate, search, execute\" design for the agent, as shown in Figure 3.1. After formulating a goal and a problem to solve, the agent calls a search procedure to solve it. It then uses the solution to guide its actions, doing whatever the solution recommends as the next thing to do-typically, the first action of the sequence-and then removing that step from the sequence. Once the solution has been executed, the agent will formulate a new goal. Open-Loop: execution\u8fc7\u7a0b\u4e2d\u4e0d\u770bpercepts. Notice that while the agent is executing the solution sequence it ignores its percepts when choosing an action because it knows in advance what they will be. An agent that carries out its plans with its eyes closed, so to speak, must be quite certain of what is going on. Control theorists call this an open-loop system, because ignoring the percepts breaks the loop between agent and environment.","title":"Problem Solving Agents"},{"location":"2-Topic%3A%20Search%20Algorithm/2.2%20Problem%20Solving%20Agents/#well-defined-problems-and-solutions","text":"A problem can be defined formally by five components: The initial state that the agent starts in A description of the possible actions available to the agent. Given a particular state \\(s\\) , ACTIONS \\((s)\\) returns the set of actions that can be executed in \\(s\\) . We say that each of these actions is applicable in \\(s\\) . A description of what each action does; the formal name for this is the transition model , specified by a function \\(\\operatorname{ResulT}(s, a)\\) that returns the state that results from doing action \\(a\\) in state \\(s\\) . We also use the term successor to refer to any state reachable from a given state by a single action. \\({ }^2\\) For example, we have Together, the initial state, actions, and transition model implicitly define the state space of the problem-the set of all states reachable from the initial state by any sequence of actions. The state space forms a directed network or graph in which the nodes are states and the links between nodes are actions. A path in the state space is a sequence of states connected by a sequence of actions. The goal test , which determines whether a given state is a goal state. Sometimes there is an explicit set of possible goal states, and the test simply checks whether the given state is one of them. A path cost function that assigns a numeric cost to each path. The problem-solving agent chooses a cost function that reflects its own performance measure. In this chapter, we assume that the cost of a path can be described as the sum of the costs of the individual actions along the path. \\({ }^3\\) The step cost of taking action \\(a\\) in state \\(s\\) to reach state \\(s^{\\prime}\\) is denoted by \\(c\\left(s, a, s^{\\prime}\\right)\\) . We assume that step costs are nonnegative. \\({ }^4\\) A solution to a problem is an action sequence that leads from the initial state to a goal state. Solution quality is measured by the path cost function, and an optimal solution has the lowest path cost among all solutions.","title":"Well-defined problems and solutions"},{"location":"2-Topic%3A%20Search%20Algorithm/2.2%20Problem%20Solving%20Agents/#formulating-problems","text":"abstraction. The process of removing detail from a representation. \u73b0\u5b9e\u4e2d\u7684\u73af\u5883\u72b6\u6001\u5305\u542b\u592a\u591a\u4fe1\u606f\uff0c \u4f46\u662f\u5f88\u591a\u4fe1\u606f\u90fd\u662f\u4e0e\u8fd9\u4e2aproblem\u65e0\u5173\u7684\u3002 \u5220\u9664\u65e0\u5173\u4fe1\u606f => abstraction. \u9664\u4e86\u62bd\u8c61\u72b6\u6001\u63cf\u8ff0\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u5fc5\u987b\u62bd\u8c61\u52a8\u4f5c\u672c\u8eab\u3002 Abstraction is valid if we can expand any abstract solution into a solution in the more detailed world; a sufficient condition is that for every detailed state that is \"in Arad,\" there is a detailed path to some state that is \"in Sibiu,\" and so on. useful if carrying out each of the actions in the solution is easier than the original problem; in this case they are easy enough that they can be carried out without further search or planning by an average driving agent. The choice of a good abstraction thus involves removing as much detail as possible while retaining validity and ensuring that the abstract actions are easy to carry out. Were it not for the ability to construct useful abstractions, intelligent agents would be completely swamped by the real world.","title":"Formulating Problems"},{"location":"2-Topic%3A%20Search%20Algorithm/2.2%20Problem%20Solving%20Agents/#searching-for-solutions","text":"A solution is an action sequence, so search algorithms work by considering various possible action sequences. The possible action sequences starting at the initial state form a search tree with the initial state at the root; the branches are actions and the nodes correspond to states in the state space of the problem. The root node of the tree corresponds to the initial state. The first step is to test whether this is a goal state. Then we need to consider taking various actions. We do this by expanding the current state; that is, applying each legal action to the current state, thereby generating a new set of states. In this case, we add three branches from the parent node \\(\\operatorname{In}(\\) Arad) leading to three new child nodes: In(Sibiu), In (Timisoara), and \\(\\operatorname{In}\\) (Zerind). Now we must choose which of these three possibilities to consider further. The set of all leaf nodes available for expansion at any given point is called the frontier The eagle-eyed reader will notice one peculiar thing about the search tree shown in Figure 3.6: it includes the path from Arad to Sibiu and back to Arad again! We say that In(Arad)is a repeated state in the search tree, generated in this case by a loopy path .","title":"Searching for Solutions"},{"location":"2-Topic%3A%20Search%20Algorithm/2.2%20Problem%20Solving%20Agents/#measure-problem-solving","text":"We can evaluate the performance of a search algorithm using the following criteria: Completeness: Is it guaranteed that the algorithm finds a solution if one exists? Optimality: Does the strategy find the optimal solution (minimum costs)? Time complexity: How long does it take to find a solution? Space complexity: How much memory is needed to perform the search?","title":"Measure Problem Solving"},{"location":"2-Topic%3A%20Search%20Algorithm/2.2%20Problem%20Solving%20Agents/#infrastructure-of-search-algorithms","text":"","title":"Infrastructure of Search Algorithms"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/","text":"Uninformed Search Summary Breath-first search https://www.wolai.com/leoxiang66/n7A2NskSk6eMr8AuNKzP1j Uniform-cost search/Dijkstra's algorithm https://www.wolai.com/leoxiang66/eVSboh5x5RPCEziLW9Azpm Depth-first search https://www.wolai.com/leoxiang66/5x38VQizFFEFF8ZKCqmxyF Depth-limited search https://www.wolai.com/leoxiang66/2p51w5BX63SvswtVwubs5C Iterative deepening search https://www.wolai.com/leoxiang66/gHcZaaibB6Ct1uckFXy5k7 Bidirectional search https://www.wolai.com/leoxiang66/sxufRXQTRAppwVS43iVDKe","title":"Uninformed Search"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/#uninformed-search","text":"","title":"Uninformed Search"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/#summary","text":"","title":"Summary"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/#breath-first-search","text":"https://www.wolai.com/leoxiang66/n7A2NskSk6eMr8AuNKzP1j","title":"Breath-first search"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/#uniform-cost-searchdijkstras-algorithm","text":"https://www.wolai.com/leoxiang66/eVSboh5x5RPCEziLW9Azpm","title":"Uniform-cost search/Dijkstra's algorithm"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/#depth-first-search","text":"https://www.wolai.com/leoxiang66/5x38VQizFFEFF8ZKCqmxyF","title":"Depth-first search"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/#depth-limited-search","text":"https://www.wolai.com/leoxiang66/2p51w5BX63SvswtVwubs5C","title":"Depth-limited search"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/#iterative-deepening-search","text":"https://www.wolai.com/leoxiang66/gHcZaaibB6Ct1uckFXy5k7","title":"Iterative deepening search"},{"location":"2-Topic%3A%20Search%20Algorithm/2.3%20Uninformed%20Search/#bidirectional-search","text":"https://www.wolai.com/leoxiang66/sxufRXQTRAppwVS43iVDKe","title":"Bidirectional search"},{"location":"2-Topic%3A%20Search%20Algorithm/2.4%20Informed%20Search/","text":"Informed Search https://www.wolai.com/leoxiang66/uSCZD5X1QeiQFCEhsg7PXc","title":"Informed Search"},{"location":"2-Topic%3A%20Search%20Algorithm/2.4%20Informed%20Search/#informed-search","text":"https://www.wolai.com/leoxiang66/uSCZD5X1QeiQFCEhsg7PXc","title":"Informed Search"}]}